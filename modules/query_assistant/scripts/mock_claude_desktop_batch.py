#!/usr/bin/env python3
"""
Mock Claude Desktop Batch Processing
Process 100 queries with batch LLM analysis
"""
import os
import json
import asyncio
import time
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path

from .mock_claude_desktop import MockClaudeDesktop
from ..mcp_server_enhanced import EnhancedIacsGraphQueryServer, EnhancedQueryRequest


async def process_queries_with_batch_llm(queries: List[str], batch_size: int = 100) -> Dict[str, Any]:
    """
    Process queries with batch LLM analysis followed by individual MCP processing
    
    Args:
        queries: List of queries to process
        batch_size: Number of queries to analyze at once with LLM (default: 100)
    """
    print(f"\nüöÄ Starting batch processing for {len(queries)} queries")
    print(f"   Batch size: {batch_size} queries per LLM call")
    
    # Initialize services
    mock = MockClaudeDesktop()
    db_config = {"type": "sqlite", "path": "data/iacsgraph.db"}
    mcp_server = EnhancedIacsGraphQueryServer(db_config=db_config)
    
    # Phase 1: Batch LLM Analysis
    print("\nüìä Phase 1: Batch LLM Analysis")
    print("-" * 60)
    
    llm_start_time = time.time()
    all_llm_results = []
    
    for i in range(0, len(queries), batch_size):
        batch_queries = queries[i:i + min(batch_size, len(queries) - i)]
        batch_num = i // batch_size + 1
        total_batches = (len(queries) + batch_size - 1) // batch_size
        
        print(f"\n[Batch {batch_num}/{total_batches}] Analyzing {len(batch_queries)} queries...")
        
        try:
            batch_start = time.time()
            batch_results = await mock.analyze_queries_batch(batch_queries, batch_size=batch_size)
            batch_time = time.time() - batch_start
            
            all_llm_results.extend(batch_results)
            
            print(f"   ‚úÖ Batch completed in {batch_time:.2f}s ({batch_time/len(batch_queries):.2f}s per query)")
            
            # Show sample result
            if batch_results:
                sample = batch_results[0]
                print(f"   Sample: '{sample.get('original_query', '')[:50]}...'")
                print(f"           Keywords: {sample.get('keywords', [])[:3]}")
                print(f"           Intent: {sample.get('intent', 'None')}")
                
        except Exception as e:
            print(f"   ‚ùå Batch failed: {e}")
            # Add fallback results for failed batch
            for query in batch_queries:
                all_llm_results.append({
                    "original_query": query,
                    "keywords": query.split(),
                    "organization": None,
                    "extracted_period": None,
                    "intent": "search",
                    "query_scope": "one",
                    "error": str(e)
                })
        
        # Rate limiting between batches
        if i + batch_size < len(queries):
            await asyncio.sleep(1)
    
    llm_total_time = time.time() - llm_start_time
    print(f"\n‚úÖ LLM Analysis Complete: {llm_total_time:.2f}s total ({llm_total_time/len(queries):.2f}s per query)")
    
    # Phase 2: MCP Processing with Pre-analyzed Parameters
    print("\nüìä Phase 2: MCP Processing")
    print("-" * 60)
    
    mcp_start_time = time.time()
    final_results = []
    success_count = 0
    error_count = 0
    
    for idx, (query, llm_result) in enumerate(zip(queries, all_llm_results)):
        try:
            # Extract parameters from LLM result
            extracted_period = llm_result.get('extracted_period')
            extracted_keywords = llm_result.get('keywords', [])
            extracted_organization = None
            
            # Get organization directly from LLM result
            extracted_organization = llm_result.get('organization')
            
            # Get query scope from LLM result
            query_scope = llm_result.get('query_scope', 'one')
            
            # Get intent from LLM result
            intent = llm_result.get('intent', 'search')
            
            # Create MCP request
            mcp_request = EnhancedQueryRequest(
                query=query,
                extracted_period=extracted_period,
                extracted_keywords=extracted_keywords,
                extracted_organization=extracted_organization,
                intent=intent,
                query_scope=query_scope,
                category=None,
                execute=True,
                limit=10,
                use_defaults=True
            )
            
            # Process with MCP
            mcp_response = await mcp_server._handle_enhanced_query(mcp_request)
            
            # Check success
            query_result = mcp_response.get('result', {})
            success = bool(query_result.query_id) and not query_result.error
            
            if success:
                success_count += 1
                status = "‚úÖ"
            else:
                error_count += 1
                status = "‚ùå"
            
            # Store result
            final_results.append({
                'index': idx + 1,
                'query': query,
                'llm_analysis': llm_result,
                'mcp_response': mcp_response,
                'success': success,
                'template_id': query_result.query_id if query_result.query_id else None,
                'error': query_result.error if hasattr(query_result, 'error') else None
            })
            
            # Progress indicator
            if (idx + 1) % 10 == 0:
                print(f"Progress: {idx + 1}/{len(queries)} - Success: {success_count}, Errors: {error_count}")
            
            # Show failures
            if not success:
                print(f"{status} [{idx + 1}] {query[:50]}... - {query_result.error if hasattr(query_result, 'error') else 'No match'}")
                
        except Exception as e:
            error_count += 1
            final_results.append({
                'index': idx + 1,
                'query': query,
                'llm_analysis': llm_result,
                'success': False,
                'error': str(e)
            })
            print(f"‚ùå [{idx + 1}] {query[:50]}... - Exception: {e}")
    
    mcp_total_time = time.time() - mcp_start_time
    total_time = time.time() - llm_start_time
    
    # Summary
    print("\n" + "="*80)
    print("BATCH PROCESSING SUMMARY")
    print("="*80)
    print(f"\n‚è±Ô∏è  Time Breakdown:")
    print(f"   LLM Batch Analysis: {llm_total_time:.2f}s ({llm_total_time/total_time*100:.1f}%)")
    print(f"   MCP Processing: {mcp_total_time:.2f}s ({mcp_total_time/total_time*100:.1f}%)")
    print(f"   Total Time: {total_time:.2f}s")
    print(f"   Average per query: {total_time/len(queries):.2f}s")
    
    print(f"\nüìä Results:")
    print(f"   Total queries: {len(queries)}")
    print(f"   Successful: {success_count} ({success_count/len(queries)*100:.1f}%)")
    print(f"   Failed: {error_count} ({error_count/len(queries)*100:.1f}%)")
    
    return {
        'summary': {
            'total_queries': len(queries),
            'success_count': success_count,
            'error_count': error_count,
            'success_rate': success_count/len(queries)*100,
            'llm_time': llm_total_time,
            'mcp_time': mcp_total_time,
            'total_time': total_time,
            'avg_time_per_query': total_time/len(queries)
        },
        'results': final_results
    }


async def test_100_queries_batch():
    """Test 100 queries with batch processing"""
    
    # 100 test queries
    test_cases = [
        # ÏïÑÏ††Îã§ Í¥ÄÎ†® (15Í∞ú)
        {"query": "ÏµúÍ∑º ÏïÑÏ††Îã§ Î™©Î°ù Î≥¥Ïó¨Ï§ò", "expected_category": "agenda"},
        {"query": "Ïñ¥Ï†ú Îì±Î°ùÎêú ÏïÑÏ††Îã§ Ï°∞Ìöå", "expected_category": "agenda"},
        {"query": "Ïò§Îäò ÏïÑÏ††Îã§ Î≠ê ÏûàÏñ¥?", "expected_category": "agenda"},
        {"query": "IMO Í¥ÄÎ†® ÏïÑÏ††Îã§ Ï∞æÏïÑÏ§ò", "expected_category": "agenda"},
        {"query": "ÌïúÍµ≠ÏÑ†Í∏â ÏïÑÏ††Îã§ Î™©Î°ù", "expected_category": "agenda"},
        {"query": "Í∏¥Í∏â ÏïÑÏ††Îã§ ÏûàÎÇòÏöî?", "expected_category": "agenda"},
        {"query": "ÎßàÍ∞êÏùº ÏûÑÎ∞ïÌïú ÏïÑÏ††Îã§", "expected_category": "agenda"},
        {"query": "ÎÇ¥ÏùºÍπåÏßÄ Ï≤òÎ¶¨Ìï¥Ïïº ÌïòÎäî ÏïÑÏ††Îã§", "expected_category": "agenda"},
        {"query": "ÏßÑÌñâÏ§ëÏù∏ ÏïÑÏ††Îã§ Î≥¥Ïó¨Ï§ò", "expected_category": "agenda"},
        {"query": "ÏôÑÎ£åÎêú ÏïÑÏ††Îã§ Î™©Î°ù", "expected_category": "agenda"},
        {"query": "Ïù¥Î≤à Ï£º ÏïÑÏ††Îã§ ÌòÑÌô©", "expected_category": "agenda"},
        {"query": "ÏßÄÎÇúÏ£º Îì±Î°ùÎêú ÏïÑÏ††Îã§Îì§", "expected_category": "agenda"},
        {"query": "ÏùòÏû•Ïù¥ ÎßåÎì† ÏïÑÏ††Îã§", "expected_category": "agenda"},
        {"query": "ÌôòÍ≤Ω Í¥ÄÎ†® ÏïÑÏ††Îã§ Ï°∞Ìöå", "expected_category": "agenda"},
        {"query": "ÏïàÏ†Ñ Í¥ÄÎ†® ÏïÑÏ††Îã§ Î™©Î°ù", "expected_category": "agenda"},
        
        # Î©îÏùº Í¥ÄÎ†® (15Í∞ú)
        {"query": "ÏùòÏû•Ïù¥ Î≥¥ÎÇ∏ Î©îÏùº Î™©Î°ù", "expected_category": "mail"},
        {"query": "Ïñ¥Ï†ú Î∞õÏùÄ Ïù¥Î©îÏùºÎì§", "expected_category": "mail"},
        {"query": "Ïò§Îäò Ïò® Î©îÏùº Î≥¥Ïó¨Ï§ò", "expected_category": "mail"},
        {"query": "IMOÏóêÏÑú Ïò® Ìé∏ÏßÄ", "expected_category": "mail"},
        {"query": "ÌïúÍµ≠ÏÑ†Í∏âÏóêÏÑú Î≥¥ÎÇ∏ Î©îÏùº", "expected_category": "mail"},
        {"query": "Í∏¥Í∏â Î©îÏùº ÏûàÏñ¥?", "expected_category": "mail"},
        {"query": "ÏùΩÏßÄ ÏïäÏùÄ Ïù¥Î©îÏùº", "expected_category": "mail"},
        {"query": "Ï§ëÏöî ÌëúÏãúÎêú Î©îÏùºÎì§", "expected_category": "mail"},
        {"query": "ÌöåÏùò Í¥ÄÎ†® Î©îÏùº Ï°∞Ìöå", "expected_category": "mail"},
        {"query": "ÏäπÏù∏ ÏöîÏ≤≠ Î©îÏùºÎì§", "expected_category": "mail"},
        {"query": "Ïù¥Î≤à Ï£º Î∞õÏùÄ Î©îÏùº", "expected_category": "mail"},
        {"query": "ÏßÄÎÇúÎã¨ Î©îÏùº ÌÜµÍ≥Ñ", "expected_category": "mail"},
        {"query": "Ï≤®Î∂ÄÌååÏùº ÏûàÎäî Î©îÏùº", "expected_category": "mail"},
        {"query": "ÎãµÏû• ÌïÑÏöîÌïú Î©îÏùºÎì§", "expected_category": "mail"},
        {"query": "Ï†ÑÏ≤¥ Î©îÏùº Î™©Î°ù Ï°∞Ìöå", "expected_category": "mail"},
        
        # Î¨∏ÏÑú/Î¨∏ÏÑúÏ†úÏ∂ú Í¥ÄÎ†® (15Í∞ú)
        {"query": "IMO Î¨∏ÏÑú Î™©Î°ù", "expected_category": "document"},
        {"query": "Ï†úÏ∂úÎêú Î¨∏ÏÑú ÌòÑÌô©", "expected_category": "document"},
        {"query": "Ïò§Îäò Ï†úÏ∂úÌïú Î¨∏ÏÑúÎì§", "expected_category": "document"},
        {"query": "Î¨∏ÏÑú Ï†úÏ∂ú ÎßàÍ∞êÏùº ÌôïÏù∏", "expected_category": "document"},
        {"query": "ÌïúÍµ≠ÏÑ†Í∏â Ï†úÏ∂ú Î¨∏ÏÑú", "expected_category": "document"},
        {"query": "ÎØ∏Ï†úÏ∂ú Î¨∏ÏÑú Î™©Î°ù", "expected_category": "document"},
        {"query": "ÏäπÏù∏Îêú Î¨∏ÏÑúÎì§ Ï°∞Ìöå", "expected_category": "document"},
        {"query": "Î∞òÎ†§Îêú Î¨∏ÏÑú ÌôïÏù∏", "expected_category": "document"},
        {"query": "Í≤ÄÌÜ†Ï§ëÏù∏ Î¨∏ÏÑú ÌòÑÌô©", "expected_category": "document"},
        {"query": "Ïù¥Î≤à Îã¨ Ï†úÏ∂ú Î¨∏ÏÑú", "expected_category": "document"},
        {"query": "ÌôòÍ≤Ω Í¥ÄÎ†® Ï†úÏ∂ú Î¨∏ÏÑú", "expected_category": "document"},
        {"query": "ÏïàÏ†Ñ Í∑úÏ†ï Î¨∏ÏÑú Î™©Î°ù", "expected_category": "document"},
        {"query": "Í∏∞Ïà† Î¨∏ÏÑú Ï†úÏ∂ú ÌòÑÌô©", "expected_category": "document"},
        {"query": "ÏúÑÏõêÌöåÎ≥Ñ Î¨∏ÏÑú ÌÜµÍ≥Ñ", "expected_category": "document"},
        {"query": "ÏµúÍ∑º ÏóÖÎç∞Ïù¥Ìä∏Îêú Î¨∏ÏÑú", "expected_category": "document"},
        
        # ÏùëÎãµ/ÏùòÍ≤¨ÏÑú Í¥ÄÎ†® (10Í∞ú)
        {"query": "ÌïúÍµ≠ÏÑ†Í∏â ÏùëÎãµ ÌòÑÌô©", "expected_category": "response"},
        {"query": "ÏùòÍ≤¨ÏÑú Ï†úÏ∂ú ÏÉÅÌÉú", "expected_category": "response"},
        {"query": "ÎØ∏ÏùëÎãµ Ìï≠Î™© Ï°∞Ìöå", "expected_category": "response"},
        {"query": "Ïò§Îäò Ï†úÏ∂úÌïú ÏùòÍ≤¨ÏÑú", "expected_category": "response"},
        {"query": "ÏùëÎãµ ÎåÄÍ∏∞Ï§ëÏù∏ Í±¥Îì§", "expected_category": "response"},
        {"query": "ÏäπÏù∏Îêú ÏùòÍ≤¨ÏÑú Î™©Î°ù", "expected_category": "response"},
        {"query": "Î∞òÎ†§Îêú ÏùëÎãµ ÌôïÏù∏", "expected_category": "response"},
        {"query": "Ïù¥Î≤à Ï£º ÏùëÎãµ ÌÜµÍ≥Ñ", "expected_category": "response"},
        {"query": "ÏúÑÏõêÌöåÎ≥Ñ ÏùëÎãµ ÌòÑÌô©", "expected_category": "response"},
        {"query": "Í∏¥Í∏â ÏùëÎãµ ÌïÑÏöî Ìï≠Î™©", "expected_category": "response"},
        
        # ÏúÑÏõêÌöå/Ï°∞ÏßÅ Í¥ÄÎ†® (10Í∞ú)
        {"query": "MSC ÏúÑÏõêÌöå Ï†ïÎ≥¥", "expected_category": "committee"},
        {"query": "MEPC ÌöåÏùò ÏùºÏ†ï", "expected_category": "committee"},
        {"query": "ÏúÑÏõêÌöåÎ≥Ñ Îã¥ÎãπÏûê Î™©Î°ù", "expected_category": "committee"},
        {"query": "ÌïúÍµ≠ÏÑ†Í∏â ÏÜåÏÜç ÏúÑÏõê", "expected_category": "committee"},
        {"query": "ÏúÑÏõêÌöå Ï∞∏ÏÑù ÌòÑÌô©", "expected_category": "committee"},
        {"query": "ÏÜåÏúÑÏõêÌöå Íµ¨ÏÑ± Ï†ïÎ≥¥", "expected_category": "committee"},
        {"query": "ÏúÑÏõêÌöåÎ≥Ñ ÏïàÍ±¥ ÌÜµÍ≥Ñ", "expected_category": "committee"},
        {"query": "Îã§Ïùå ÌöåÏùò ÏùºÏ†ï ÌôïÏù∏", "expected_category": "committee"},
        {"query": "ÏúÑÏõêÌöå ÏùòÏû• Ï†ïÎ≥¥", "expected_category": "committee"},
        {"query": "ÏûëÏóÖÎ∞ò Íµ¨ÏÑ±Ïõê Ï°∞Ìöå", "expected_category": "committee"},
        
        # ÏùºÏ†ï/ÎßàÍ∞êÏùº Í¥ÄÎ†® (10Í∞ú)
        {"query": "Ïò§Îäò ÏùºÏ†ï ÌôïÏù∏", "expected_category": "schedule"},
        {"query": "ÎÇ¥Ïùº ÏòàÏ†ïÎêú ÌöåÏùò", "expected_category": "schedule"},
        {"query": "Ïù¥Î≤à Ï£º ÎßàÍ∞êÏùº", "expected_category": "schedule"},
        {"query": "Îã§Ïùå Îã¨ Ï£ºÏöî ÏùºÏ†ï", "expected_category": "schedule"},
        {"query": "Ïó∞Í∞Ñ ÌöåÏùò Í≥ÑÌöç", "expected_category": "schedule"},
        {"query": "ÎßàÍ∞ê ÏûÑÎ∞ï Ìï≠Î™©Îì§", "expected_category": "schedule"},
        {"query": "ÏßÄÏó∞Îêú ÏùºÏ†ï ÌôïÏù∏", "expected_category": "schedule"},
        {"query": "Ìú¥Ïùº ÏùºÏ†ï Ï°∞Ìöå", "expected_category": "schedule"},
        {"query": "Î∞òÎ≥µ ÏùºÏ†ï Î™©Î°ù", "expected_category": "schedule"},
        {"query": "ÏùºÏ†ï Ï∂©Îèå ÌôïÏù∏", "expected_category": "schedule"},
        
        # ÌÜµÍ≥Ñ/Î∂ÑÏÑù Í¥ÄÎ†® (10Í∞ú)
        {"query": "ÏõîÎ≥Ñ Î¨∏ÏÑú Ï†úÏ∂ú ÌÜµÍ≥Ñ", "expected_category": "statistics"},
        {"query": "ÏúÑÏõêÌöåÎ≥Ñ ÌôúÎèô Î∂ÑÏÑù", "expected_category": "statistics"},
        {"query": "ÏùëÎãµÎ•† ÌÜµÍ≥Ñ Ï°∞Ìöå", "expected_category": "statistics"},
        {"query": "ÏïÑÏ††Îã§ Ï≤òÎ¶¨ ÌòÑÌô©", "expected_category": "statistics"},
        {"query": "Î©îÏùº ÏàòÏã† ÌÜµÍ≥Ñ", "expected_category": "statistics"},
        {"query": "Ï∞∏Ïó¨Ïú® Î∂ÑÏÑù Î≥¥Í≥†", "expected_category": "statistics"},
        {"query": "Ïó∞ÎèÑÎ≥Ñ Ïã§Ï†Å ÎπÑÍµê", "expected_category": "statistics"},
        {"query": "Î∂ÄÏÑúÎ≥Ñ ÏÑ±Í≥º ÏßÄÌëú", "expected_category": "statistics"},
        {"query": "ÌîÑÎ°úÏ†ùÌä∏ ÏßÑÌñâÎ•†", "expected_category": "statistics"},
        {"query": "Î¶¨Ïä§ÌÅ¨ Î∂ÑÏÑù ÌòÑÌô©", "expected_category": "statistics"},
        
        # Í≤ÄÏÉâ/Ï°∞Ìöå Í¥ÄÎ†® (10Í∞ú)
        {"query": "ÍπÄÏ≤†ÏàòÍ∞Ä ÏûëÏÑ±Ìïú Î¨∏ÏÑú", "expected_category": "search"},
        {"query": "ÌôòÍ≤Ω Í∑úÏ†ú Í¥ÄÎ†® ÏûêÎ£å", "expected_category": "search"},
        {"query": "2024ÎÖÑ ÌöåÏùòÎ°ù Ï∞æÍ∏∞", "expected_category": "search"},
        {"query": "ÏäπÏù∏ ÎåÄÍ∏∞ Ìï≠Î™©Îì§", "expected_category": "search"},
        {"query": "ÏµúÍ∑º Î≥ÄÍ≤ΩÏÇ¨Ìï≠ Ï°∞Ìöå", "expected_category": "search"},
        {"query": "ÌÇ§ÏõåÎìúÎ°ú Î¨∏ÏÑú Í≤ÄÏÉâ", "expected_category": "search"},
        {"query": "Îã¥ÎãπÏûêÎ≥Ñ ÏóÖÎ¨¥ ÌòÑÌô©", "expected_category": "search"},
        {"query": "ÌîÑÎ°úÏ†ùÌä∏ Í¥ÄÎ†® ÏûêÎ£å", "expected_category": "search"},
        {"query": "Í∑úÏ†ï Î≥ÄÍ≤Ω Ïù¥Î†•", "expected_category": "search"},
        {"query": "Ï∞∏Ï°∞ Î¨∏ÏÑú Î™©Î°ù", "expected_category": "search"},
        
        # ÏïåÎ¶º/Î¶¨ÎßàÏù∏Îçî (5Í∞ú)
        {"query": "Ïò§Îäò ÏïåÎ¶º Î™©Î°ù", "expected_category": "notification"},
        {"query": "ÎßàÍ∞êÏùº Î¶¨ÎßàÏù∏Îçî", "expected_category": "notification"},
        {"query": "Ï§ëÏöî Í≥µÏßÄÏÇ¨Ìï≠", "expected_category": "notification"},
        {"query": "ÏãúÏä§ÌÖú ÏïåÎ¶º ÌôïÏù∏", "expected_category": "notification"},
        {"query": "ÏóÖÎç∞Ïù¥Ìä∏ ÏïåÎ¶º", "expected_category": "notification"}
    ]
    
    queries = [tc['query'] for tc in test_cases]
    
    print("="*80)
    print(f"Testing {len(queries)} queries with Batch LLM Processing")
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # Process with 100 queries at once
    batch_size = 100
    print(f"\n\n{'='*80}")
    print(f"Testing with batch size: {batch_size}")
    print(f"{'='*80}")
    
    result = await process_queries_with_batch_llm(queries, batch_size=batch_size)
    
    # Save results
    output_file = f'mock_claude_batch_results_{batch_size}.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'test_date': datetime.now().isoformat(),
            'batch_size': batch_size,
            'summary': result['summary'],
            'detailed_results': result['results']
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìÑ Results saved to: {output_file}")


async def test_sample_batch():
    """Test with a small sample for verification"""
    sample_queries = [
        "ÏµúÍ∑º ÏïÑÏ††Îã§ Î™©Î°ù Î≥¥Ïó¨Ï§ò",
        "ÌïúÍµ≠ÏÑ†Í∏â ÏùëÎãµ ÌòÑÌô©",
        "Ïñ¥Ï†ú Î∞õÏùÄ Ïù¥Î©îÏùºÎì§",
        "IMO Í¥ÄÎ†® Î¨∏ÏÑú",
        "ÏßÄÎÇúÏ£º Îì±Î°ùÎêú ÏïÑÏ††Îã§Îì§"
    ]
    
    print("="*80)
    print(f"Testing {len(sample_queries)} sample queries with Batch Processing")
    print("="*80)
    
    result = await process_queries_with_batch_llm(sample_queries, batch_size=5)
    
    # Show detailed results for samples
    print("\nüìã Detailed Results:")
    for r in result['results']:
        print(f"\n[{r['index']}] {r['query']}")
        print(f"   Success: {r['success']}")
        if r['success']:
            print(f"   Template: {r.get('template_id', 'N/A')}")
        else:
            print(f"   Error: {r.get('error', 'Unknown')}")
        
        llm = r.get('llm_analysis', {})
        print(f"   Keywords: {llm.get('keywords', [])}")
        print(f"   Organization: {llm.get('organization', 'None')}")


async def main():
    """Main function with command line options"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Mock Claude Desktop Batch Processing')
    parser.add_argument('--sample', action='store_true',
                       help='Test with 5 sample queries instead of 100')
    parser.add_argument('--batch-size', type=int, default=100,
                       help='Number of queries per LLM batch (default: 100)')
    
    args = parser.parse_args()
    
    # Check API key
    if not os.getenv("OPENROUTER_API_KEY"):
        print("‚ùå OPENROUTER_API_KEY not found in environment")
        print("Please set it in .env file or export it")
        return
    
    try:
        if args.sample:
            await test_sample_batch()
        else:
            # Always use test_100_queries_batch
            await test_100_queries_batch()
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())